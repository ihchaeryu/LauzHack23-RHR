{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import umap\n",
    "from kneed import KneeLocator\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, centroid\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import confusion_matrix, classification_report, silhouette_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible fonts: DejaVuSans-Bold.ttf, DejaVuSans-BoldOblique.ttf, DejaVuSans-ExtraLight.ttf, DejaVuSans-Oblique.ttf, DejaVuSans.ttf, DejaVuSansCondensed-Bold.ttf, DejaVuSansCondensed-BoldOblique.ttf, DejaVuSansCondensed-Oblique.ttf, DejaVuSansCondensed.ttf, DejaVuSansMono-Bold.ttf, DejaVuSansMono-BoldOblique.ttf, DejaVuSansMono-Oblique.ttf, DejaVuSansMono.ttf, GillSansBoItNova.ttf, GillSansBoNova.ttf, GillSansCondBoItNova.ttf, GillSansCondBoNova.ttf, GillSansCondExtraItNova.ttf, GillSansCondExtraNova.ttf, GillSansCondItNova.ttf, GillSansCondLightItNova.ttf, GillSansCondLightNova.ttf, GillSansCondNova.ttf, GillSansCondUltraBoNova.ttf, GillSansItNova.ttf, GillSansLightItNova.ttf, GillSansLightNova.ttf, GillSansNova.ttf, GillSansUltraBoNova.ttf, SansSerifCollection.ttf, \n"
     ]
    }
   ],
   "source": [
    "url      = 'https://github.com/MaartenGr/cTFIDF/archive/refs/tags/v0.1.1.tar.gz'\n",
    "version  = re.search(r'/v(.+?)\\.tar\\.gz', url).group(1)\n",
    "dir_name = f'cTFIDF-{version}'\n",
    "dir_path = os.path.join(os.getcwd(),dir_name)\n",
    "import sys\n",
    "if sys.path[-1] != dir_path:\n",
    "    sys.path.append(dir_path)\n",
    "from ctfidf import CTFIDFVectorizer\n",
    "\n",
    "import matplotlib.font_manager\n",
    "fonts = matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "print(\"Possible fonts: \", end=\"\")\n",
    "for f in sorted(fonts):\n",
    "    if 'Narrow' in f:\n",
    "        print(f.split(os.path.sep)[-1], end=\", \")\n",
    "    elif 'Sans' in f:\n",
    "        print(f.split(os.path.sep)[-1], end=\", \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using font: C:\\Windows\\Fonts\\Candaral.ttf\n",
      "  Guessing at font name: C : \\ Windows \\ Fonts \\ Candaral .ttf\n"
     ]
    }
   ],
   "source": [
    "fp = fonts[0] #Â Ensure at least _something_ is set here\n",
    "for f in fonts:\n",
    "    if 'LiberationSansNarrow-Regular' in f:\n",
    "        fp = f.split(os.path.sep)[-1].split('.')[0]\n",
    "        break\n",
    "    elif 'Arial Narrow.ttf' in f:\n",
    "        fp = f.split(os.path.sep)[-1].split('.')[0]\n",
    "        break\n",
    "    elif 'Narrow' in f:\n",
    "        fp = f.split(os.path.sep)[-1].split('.')[0]\n",
    "print(f\"Using font: {fp}\")\n",
    "\n",
    "fname = ''.join([f' {x}' if x==x.upper() else x for x in fp.split('-')[0]]).strip().replace('  ','')\n",
    "print(f\"  Guessing at font name: {fname}\")\n",
    "\n",
    "# These are font dictionaries for the 's'uper-title, 't'itle, \n",
    "# 'a'xis, and 'l'abels.\n",
    "sfont = {'fontname':fname, 'fontsize':16}\n",
    "tfont = {'fontname':fname, 'fontsize':12}\n",
    "afont = {'fontname':fname, 'fontsize':10}\n",
    "lfont = {'fontname':fname, 'fontsize':8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "rs = 43\n",
    "\n",
    "# Which embeddings to use\n",
    "src_embeddings = 'doc_vec'  #'word_vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the file\n",
    "fn = 'Amazon_Unlocked_Mobile.csv'\n",
    "\n",
    "# See if the data has already been downloaded, and\n",
    "# if not, download it from the web site. We save a\n",
    "# copy locally so that you can run this tutorial\n",
    "# offline and also spare the host the bandwidth costs\n",
    "if os.path.exists(os.path.join('data',fn)):\n",
    "    df = pd.read_csv(os.path.join('data',fn))\n",
    "else:\n",
    "    # We will look for/create a 'data' directory\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "   \n",
    "    # Download and save\n",
    "    df = pd.read_parquet(f'http://orca.casa.ucl.ac.uk/~jreades/data/{fn}')\n",
    "    df.to_parquet(os.path.join('data',fn))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading columns: Product Name, Brand Name, Price, Rating, Reviews, Review Votes\n",
      "               Price         Rating   Review Votes\n",
      "count  407907.000000  413840.000000  401544.000000\n",
      "mean      226.867155       3.819578       1.507237\n",
      "std       273.006259       1.548216       9.163853\n",
      "min         1.730000       1.000000       0.000000\n",
      "25%        79.990000       3.000000       0.000000\n",
      "50%       144.710000       5.000000       0.000000\n",
      "75%       269.990000       5.000000       1.000000\n",
      "max      2598.000000       5.000000     645.000000\n",
      "413840\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading columns: {', '.join(df.columns.tolist())}\")\n",
    "print(df.describe())\n",
    "print(df.shape[0])\n",
    "\n",
    "df_small = df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading columns: Product Name, Brand Name, Price, Rating, Reviews, Review Votes, Summary\n",
      "                                        Product Name Brand Name   Price  \\\n",
      "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
      "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
      "\n",
      "   Rating                                            Reviews  Review Votes  \\\n",
      "0       5  I feel so LUCKY to have found this used (phone...             1   \n",
      "1       4  nice phone, nice up grade from my pantach revu...             0   \n",
      "\n",
      "                                             Summary  \n",
      "0  [('positive', 'found this used phone in good c...  \n",
      "1  [('positive', 'nice upgrade from my Pantach Re...  \n"
     ]
    }
   ],
   "source": [
    "#Load open ai data. \n",
    "\n",
    "# Name of the file\n",
    "fn = 'reviews_summaries.xlsx'\n",
    "\n",
    "if os.path.exists(os.path.join('data',fn)):\n",
    "    df_ai = pd.read_excel(os.path.join('data',fn))\n",
    "    df_ai.to_csv('reviews_summaries.csv', index=False) # in place\n",
    "\n",
    "print(f\"Loading columns: {', '.join(df_ai.columns.tolist())}\")\n",
    "# print(df.describe())\n",
    "print(df_ai.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Product Name', 'Brand Name', 'Price', 'Rating', 'Reviews',\n",
      "       'Review Votes', 'Summary'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_ai.columns)\n",
    "# df_ai[[\"Reviews\", \"Summary\"]]\n",
    "# df_ai[[\"Reviews\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'samsung' and 'galaxy' - CBOW :  -0.18181361\n",
      "Cosine similarity between 'samsung' and 'great' - CBOW :  0.2900546\n",
      "Cosine similarity between 'samsung' and 'galaxy' - Skip Gram :  0.7133872\n",
      "Cosine similarity between 'samsung' and 'great' - Skip Gram :  0.908905\n"
     ]
    }
   ],
   "source": [
    "# Python program to generate word vectors using Word2Vec\n",
    " \n",
    "# importing all necessary modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    " \n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    " \n",
    "data_reviews = []\n",
    "# iterate through each sentence in the file\n",
    "for i in df_ai[[\"Reviews\"]].to_numpy():\n",
    "    \n",
    "    temp = []\n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i[0]):\n",
    "        temp.append(j.lower())\n",
    " \n",
    "    data_reviews.append(temp)\n",
    " \n",
    "# Create CBOW model\n",
    "model1 = gensim.models.Word2Vec(data_reviews, min_count = 1, \n",
    "                              vector_size = 100, window = 5)\n",
    " \n",
    "# Print results\n",
    "print(\"Cosine similarity between 'samsung' \" +\n",
    "               \"and 'galaxy' - CBOW : \",\n",
    "    model1.wv.similarity('samsung', 'galaxy'))\n",
    "     \n",
    "print(\"Cosine similarity between 'samsung' \" +\n",
    "                 \"and 'great' - CBOW : \",\n",
    "      model1.wv.similarity('samsung', 'great'))\n",
    " \n",
    "# Create Skip Gram model\n",
    "model2 = gensim.models.Word2Vec(data_reviews, min_count = 1, vector_size = 100,\n",
    "                                             window = 5, sg = 1)\n",
    " \n",
    "# Print results\n",
    "print(\"Cosine similarity between 'samsung' \" +\n",
    "          \"and 'galaxy' - Skip Gram : \",\n",
    "    model2.wv.similarity('samsung', 'galaxy'))\n",
    "     \n",
    "print(\"Cosine similarity between 'samsung' \" +\n",
    "            \"and 'great' - Skip Gram : \",\n",
    "      model2.wv.similarity('samsung', 'great'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary = []\n",
    "# iterate through each sentence in the file\n",
    "for i in df_ai[[\"Summary\"]].to_numpy():\n",
    "    \n",
    "    temp = []\n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i[0]):\n",
    "        temp.append(j.lower())\n",
    " \n",
    "    data_summary.append(temp)\n",
    "\n",
    "# Create CBOW model\n",
    "model1_summary = gensim.models.Word2Vec(data_summary, min_count = 1, \n",
    "                              vector_size = 100, window = 5)\n",
    " \n",
    " \n",
    "# Create Skip Gram model\n",
    "model2_summary = gensim.models.Word2Vec(data_summary, min_count = 1, vector_size = 100,\n",
    "                                             window = 5, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "from gensim.models import word2vec,KeyedVectors \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import gensim.downloader\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "glove_vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key '2.5+' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gabri\\OneDrive - Danmarks Tekniske Universitet\\Lokale Filer\\DTU\\EfterÃ¥r 2023 (EPFL)\\LauzHack\\LauzHack23-RHR\\Gabriel_NLP\\AWS.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         embeddings\u001b[39m.\u001b[39mappend(review_embedding(text, model \u001b[39m=\u001b[39m glove_vectors))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39m# tokenize the sentence into words\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# embeddings_summary = embedding_list(df_ai, \"Summary\") # requires more segmenting.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# df_ai[\"embeddings_summary\"] = embeddings_summary\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m embeddings_reviews \u001b[39m=\u001b[39m embedding_list(df_ai, \u001b[39m\"\u001b[39;49m\u001b[39mReviews\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m# A bunch of weird characters in so doesn't quite work.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m df_ai[\u001b[39m\"\u001b[39m\u001b[39membeddings_reviews\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m embeddings_reviews\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# fn = \"embeddings.csv\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# embeddings = pd.read_csv(os.path.join('data',fn))[\"embedding\"]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# df_ai_embedded = pd.concat([df_ai, embeddings], axis=1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# df_ai_embedded[\"embedding\"]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# df_ai_embedded[\"embedding\"].iloc[0,]\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\gabri\\OneDrive - Danmarks Tekniske Universitet\\Lokale Filer\\DTU\\EfterÃ¥r 2023 (EPFL)\\LauzHack\\LauzHack23-RHR\\Gabriel_NLP\\AWS.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m text \u001b[39m=\u001b[39m text[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m embeddings\u001b[39m.\u001b[39mappend(review_embedding(text, model \u001b[39m=\u001b[39;49m glove_vectors))\n",
      "\u001b[1;32mc:\\Users\\gabri\\OneDrive - Danmarks Tekniske Universitet\\Lokale Filer\\DTU\\EfterÃ¥r 2023 (EPFL)\\LauzHack\\LauzHack23-RHR\\Gabriel_NLP\\AWS.ipynb Cell 12\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mean \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m word_tokenize(text):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     mean \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m model[j\u001b[39m.\u001b[39;49mlower()]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gabri/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/Lokale%20Filer/DTU/Efter%C3%A5r%202023%20%28EPFL%29/LauzHack/LauzHack23-RHR/Gabriel_NLP/AWS.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mreturn\u001b[39;00m mean\n",
      "File \u001b[1;32mc:\\Users\\gabri\\miniconda3\\envs\\lauzHackNLP\\lib\\site-packages\\gensim\\models\\keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[1;34m(self, key_or_keys)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[0;32m    391\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[1;32m--> 403\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_vector(key_or_keys)\n\u001b[0;32m    405\u001b[0m \u001b[39mreturn\u001b[39;00m vstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_vector(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m key_or_keys])\n",
      "File \u001b[1;32mc:\\Users\\gabri\\miniconda3\\envs\\lauzHackNLP\\lib\\site-packages\\gensim\\models\\keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_vector\u001b[39m(\u001b[39mself\u001b[39m, key, norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    423\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \n\u001b[0;32m    425\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    444\u001b[0m \n\u001b[0;32m    445\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_index(key)\n\u001b[0;32m    447\u001b[0m     \u001b[39mif\u001b[39;00m norm:\n\u001b[0;32m    448\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfill_norms()\n",
      "File \u001b[1;32mc:\\Users\\gabri\\miniconda3\\envs\\lauzHackNLP\\lib\\site-packages\\gensim\\models\\keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n\u001b[0;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key '2.5+' not present\""
     ]
    }
   ],
   "source": [
    "# glove_vectors[\"samsung\"]\n",
    "def review_embedding(text, model = glove_vectors):\n",
    "    mean = 0\n",
    "    for j in word_tokenize(text):\n",
    "        mean += model[j.lower()]\n",
    "    return mean\n",
    "\n",
    "def embedding_list(df, col, model = glove_vectors):\n",
    "    for text in df[[col]].to_numpy():\n",
    "        text = text[0]\n",
    "        embeddings = []\n",
    "\n",
    "        embeddings.append(review_embedding(text, model = glove_vectors))\n",
    "        # tokenize the sentence into words\n",
    "\n",
    "\n",
    "# embeddings_summary = embedding_list(df_ai, \"Summary\") # requires more segmenting.\n",
    "# df_ai[\"embeddings_summary\"] = embeddings_summary\n",
    "\n",
    "embeddings_reviews = embedding_list(df_ai, \"Reviews\") # A bunch of weird characters in so doesn't quite work.\n",
    "df_ai[\"embeddings_reviews\"] = embeddings_reviews\n",
    "\n",
    "# fn = \"embeddings.csv\"\n",
    "# embeddings = pd.read_csv(os.path.join('data',fn))[\"embedding\"]\n",
    "# df_ai_embedded = pd.concat([df_ai, embeddings], axis=1)\n",
    "# df_ai_embedded[\"embedding\"]\n",
    "# df_ai_embedded[\"embedding\"].iloc[0,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP dimensionality reduction to 4 dimensions with 'euclidean' distance measure.\n"
     ]
    }
   ],
   "source": [
    "dmeasure = 'euclidean'\n",
    "rdims    = 4 # r-dims == Reduced dimensionality\n",
    "print(f\"UMAP dimensionality reduction to {rdims} dimensions with '{dmeasure}' distance measure.\")\n",
    "\n",
    "\n",
    "def x_from_df(df:pd.DataFrame, col:str='Embedding') -> pd.DataFrame:\n",
    "    cols = ['E'+str(x) for x in np.arange(0,len(df[col].iloc[0]))]\n",
    "    return pd.DataFrame(df[col].tolist(), columns=cols, index=df.index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lauzHackNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
